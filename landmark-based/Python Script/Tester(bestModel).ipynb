{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14331501,"sourceType":"datasetVersion","datasetId":9149704}],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchvision\nimport cv2\nimport numpy as np\nimport albumentations as A\nfrom skimage import io\nimport pandas as pd\n\nprint(\"Torch:\", torch.__version__)\nprint(\"Torchvision:\", torchvision.__version__)\nprint(\"OpenCV:\", cv2.__version__)\nprint(\"Albumentations:\", A.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T12:26:38.679258Z","iopub.execute_input":"2025-12-29T12:26:38.679514Z","iopub.status.idle":"2025-12-29T12:26:38.684923Z","shell.execute_reply.started":"2025-12-29T12:26:38.679498Z","shell.execute_reply":"2025-12-29T12:26:38.684148Z"}},"outputs":[{"name":"stdout","text":"Torch: 2.6.0+cu124\nTorchvision: 0.21.0+cu124\nOpenCV: 4.12.0\nAlbumentations: 2.0.8\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"IMG_SIZE = 256        \nNUM_LANDMARKS = 4     \nHEATMAP_SIZE = 256    \nGAUSSIAN_RADIUS = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T12:26:38.686063Z","iopub.execute_input":"2025-12-29T12:26:38.686253Z","iopub.status.idle":"2025-12-29T12:26:38.700269Z","shell.execute_reply.started":"2025-12-29T12:26:38.686239Z","shell.execute_reply":"2025-12-29T12:26:38.699561Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"import numpy as np\nimport cv2\n\ndef generate_heatmap(h, w, x, y, radius=4):\n    heatmap = np.zeros((h, w), dtype=np.float32)\n    \n    x = int(x)\n    y = int(y)\n\n    if x < 0 or y < 0 or x >= w or y >= h:\n        return heatmap  \n    \n    cv2.circle(heatmap, (x, y), radius, 1.0, -1)\n    heatmap = cv2.GaussianBlur(heatmap, (0,0), sigmaX=radius, sigmaY=radius)\n    heatmap = heatmap / heatmap.max() if heatmap.max() > 0 else heatmap\n    return heatmap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T12:26:38.700972Z","iopub.execute_input":"2025-12-29T12:26:38.701188Z","iopub.status.idle":"2025-12-29T12:26:38.714826Z","shell.execute_reply.started":"2025-12-29T12:26:38.701173Z","shell.execute_reply":"2025-12-29T12:26:38.714237Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"import pandas as pd\n\nval_df = pd.read_csv(\"/kaggle/input/trainertaska-1/validation_split.csv\")\nVAL_IMG_DIR = \"/kaggle/input/trainertaska-1/val_images_zip\"\n\nlen(val_df), VAL_IMG_DIR ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T12:26:38.716450Z","iopub.execute_input":"2025-12-29T12:26:38.716932Z","iopub.status.idle":"2025-12-29T12:26:38.736631Z","shell.execute_reply.started":"2025-12-29T12:26:38.716913Z","shell.execute_reply":"2025-12-29T12:26:38.735943Z"}},"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"(94, '/kaggle/input/trainertaska-1/val_images_zip')"},"metadata":{}}],"execution_count":79},{"cell_type":"code","source":"def resize_with_landmarks(img, landmarks):\n    h, w = img.shape[:2]\n\n    img_resized = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n\n    scale_x = IMG_SIZE / w\n    scale_y = IMG_SIZE / h\n\n    scaled = []\n    for x, y in landmarks:\n        scaled.append((x * scale_x, y * scale_y))\n\n    return img_resized, scaled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T12:26:38.737262Z","iopub.execute_input":"2025-12-29T12:26:38.737433Z","iopub.status.idle":"2025-12-29T12:26:38.742180Z","shell.execute_reply.started":"2025-12-29T12:26:38.737419Z","shell.execute_reply":"2025-12-29T12:26:38.741479Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass FetalLandmarkDataset(Dataset):\n    def __init__(self, df, img_dir, augment=False):\n        self.df = df.reset_index(drop=True)\n        self.img_dir = img_dir\n        self.augment = augment\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        try:\n            row = self.df.iloc[idx]\n            img_path = os.path.join(self.img_dir, row['image_name'])\n\n            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n            if img is None:\n                raise ValueError(\"Image failed to load\")\n\n            landmarks = [\n                (row['ofd_1_x'], row['ofd_1_y']),\n                (row['ofd_2_x'], row['ofd_2_y']),\n                (row['bpd_1_x'], row['bpd_1_y']),\n                (row['bpd_2_x'], row['bpd_2_y'])\n            ]\n\n            if self.augment:\n                augmented = train_augs(image=img, keypoints=landmarks)\n                img = augmented['image']\n                landmarks = augmented['keypoints']\n\n            img, landmarks = resize_with_landmarks(img, landmarks)\n            img = np.ascontiguousarray(img).astype(np.float32) / 255.0\n\n            img = np.stack([img, img, img], axis=0)\n\n     \n            landmarks = list(landmarks)\n\n            if len(landmarks) > 4:\n                landmarks = landmarks[:4]\n\n            while len(landmarks) < 4:\n                landmarks.append((None, None))\n\n            heatmaps = []\n            for (x, y) in landmarks:\n                if (\n                    x is None or y is None or\n                    np.isnan(x) or np.isnan(y)\n                ):\n                    hm = np.zeros((HEATMAP_SIZE, HEATMAP_SIZE), dtype=np.float32)\n                else:\n                    hm = generate_heatmap(\n                        HEATMAP_SIZE,\n                        HEATMAP_SIZE,\n                        int(x),\n                        int(y),\n                        GAUSSIAN_RADIUS\n                    )\n                heatmaps.append(hm)\n\n            heatmaps = np.stack(heatmaps, axis=0).astype(np.float32)\n            heatmaps = np.ascontiguousarray(heatmaps)\n\n            return (\n                torch.from_numpy(img.copy()).float(),\n                torch.from_numpy(heatmaps.copy()).float()\n            )\n\n        except Exception as e:\n            print(\"BAD SAMPLE:\", idx, self.df.iloc[idx]['image_name'], e)\n\n            dummy_img = np.zeros((3, 256, 256), dtype=np.float32)\n            dummy_hm  = np.zeros((4, 256, 256), dtype=np.float32)\n\n            return (\n                torch.from_numpy(dummy_img).float(),\n                torch.from_numpy(dummy_hm).float()\n            )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T12:26:38.742955Z","iopub.execute_input":"2025-12-29T12:26:38.743118Z","iopub.status.idle":"2025-12-29T12:26:38.759991Z","shell.execute_reply.started":"2025-12-29T12:26:38.743105Z","shell.execute_reply":"2025-12-29T12:26:38.759367Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nval_dataset = FetalLandmarkDataset(val_df, VAL_IMG_DIR, augment=False)\nval_loader  = DataLoader(val_dataset, batch_size=8, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T12:26:38.760634Z","iopub.execute_input":"2025-12-29T12:26:38.760796Z","iopub.status.idle":"2025-12-29T12:26:38.779716Z","shell.execute_reply.started":"2025-12-29T12:26:38.760782Z","shell.execute_reply":"2025-12-29T12:26:38.779045Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nclass Hourglass(nn.Module):\n    def __init__(self, depth, channels):\n        super().__init__()\n        self.depth = depth\n        self.channels = channels\n\n        self.res = ConvBlock(channels, channels)\n\n        if depth > 1:\n            self.down = DownSample(channels)\n            self.inner = Hourglass(depth-1, channels)\n            self.up = UpSample(channels)\n        else:\n            self.inner_res = ConvBlock(channels, channels)\n\n    def forward(self, x):\n        up1 = self.res(x)\n\n        if self.depth > 1:\n            low = self.down(x)\n            low = self.inner(low)\n            low = self.up(low)\n        else:\n            low = self.inner_res(x)\n\n        return up1 + low\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T12:26:38.798464Z","iopub.execute_input":"2025-12-29T12:26:38.799131Z","iopub.status.idle":"2025-12-29T12:26:38.804633Z","shell.execute_reply.started":"2025-12-29T12:26:38.799108Z","shell.execute_reply":"2025-12-29T12:26:38.803890Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch)\n        )\n        self.skip = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n\n    def forward(self, x):\n        return torch.relu(self.conv(x) + self.skip(x))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T12:26:38.805906Z","iopub.execute_input":"2025-12-29T12:26:38.806138Z","iopub.status.idle":"2025-12-29T12:26:38.820972Z","shell.execute_reply.started":"2025-12-29T12:26:38.806118Z","shell.execute_reply":"2025-12-29T12:26:38.820289Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"class DownSample(nn.Module):\n    def __init__(self, ch):\n        super().__init__()\n        self.pool = nn.MaxPool2d(2)\n        self.conv = ConvBlock(ch, ch)\n\n    def forward(self, x):\n        return self.conv(self.pool(x))\n\n\nclass UpSample(nn.Module):\n    def __init__(self, ch):\n        super().__init__()\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.conv = ConvBlock(ch, ch)\n\n    def forward(self, x):\n        return self.conv(self.up(x))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T12:26:38.821659Z","iopub.execute_input":"2025-12-29T12:26:38.821870Z","iopub.status.idle":"2025-12-29T12:26:38.836276Z","shell.execute_reply.started":"2025-12-29T12:26:38.821854Z","shell.execute_reply":"2025-12-29T12:26:38.835582Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"class HeatmapHead(nn.Module):\n    def __init__(self, ch, num_landmarks):\n        super().__init__()\n        self.head = nn.Sequential(\n            nn.Conv2d(ch, ch, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(ch, num_landmarks, 1)\n        )\n\n    def forward(self, x):\n        return self.head(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T12:26:38.837394Z","iopub.execute_input":"2025-12-29T12:26:38.837668Z","iopub.status.idle":"2025-12-29T12:26:38.851236Z","shell.execute_reply.started":"2025-12-29T12:26:38.837645Z","shell.execute_reply":"2025-12-29T12:26:38.850577Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"class HourglassNet(nn.Module):\n    def __init__(self, num_landmarks=4, channels=64, depth=4):\n        super().__init__()\n\n        self.pre = nn.Sequential(\n            ConvBlock(3, channels),\n            ConvBlock(channels, channels)\n        )\n\n        self.hourglass = Hourglass(depth, channels)\n        self.head = HeatmapHead(channels, num_landmarks)\n\n    def forward(self, x):\n        x = self.pre(x)\n        x = self.hourglass(x)\n        x = self.head(x)\n        return x ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T12:26:38.852628Z","iopub.execute_input":"2025-12-29T12:26:38.852801Z","iopub.status.idle":"2025-12-29T12:26:38.871140Z","shell.execute_reply.started":"2025-12-29T12:26:38.852787Z","shell.execute_reply":"2025-12-29T12:26:38.870433Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"model = HourglassNet(num_landmarks=4).cuda()\nmodel.load_state_dict(torch.load(\"/kaggle/input/trainertaska-1/best_hourglass.pth\"))\nmodel.eval() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T12:26:38.871831Z","iopub.execute_input":"2025-12-29T12:26:38.872021Z","iopub.status.idle":"2025-12-29T12:26:38.946687Z","shell.execute_reply.started":"2025-12-29T12:26:38.872006Z","shell.execute_reply":"2025-12-29T12:26:38.946018Z"}},"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"HourglassNet(\n  (pre): Sequential(\n    (0): ConvBlock(\n      (conv): Sequential(\n        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (skip): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    )\n    (1): ConvBlock(\n      (conv): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (skip): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    )\n  )\n  (hourglass): Hourglass(\n    (res): ConvBlock(\n      (conv): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (skip): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    )\n    (down): DownSample(\n      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (conv): ConvBlock(\n        (conv): Sequential(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (skip): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      )\n    )\n    (inner): Hourglass(\n      (res): ConvBlock(\n        (conv): Sequential(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (skip): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      )\n      (down): DownSample(\n        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (conv): ConvBlock(\n          (conv): Sequential(\n            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (skip): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        )\n      )\n      (inner): Hourglass(\n        (res): ConvBlock(\n          (conv): Sequential(\n            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (skip): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        )\n        (down): DownSample(\n          (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n          (conv): ConvBlock(\n            (conv): Sequential(\n              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): ReLU(inplace=True)\n              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n            (skip): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          )\n        )\n        (inner): Hourglass(\n          (res): ConvBlock(\n            (conv): Sequential(\n              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): ReLU(inplace=True)\n              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n            (skip): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          )\n          (inner_res): ConvBlock(\n            (conv): Sequential(\n              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): ReLU(inplace=True)\n              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n            (skip): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          )\n        )\n        (up): UpSample(\n          (up): Upsample(scale_factor=2.0, mode='bilinear')\n          (conv): ConvBlock(\n            (conv): Sequential(\n              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): ReLU(inplace=True)\n              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n            (skip): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          )\n        )\n      )\n      (up): UpSample(\n        (up): Upsample(scale_factor=2.0, mode='bilinear')\n        (conv): ConvBlock(\n          (conv): Sequential(\n            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (skip): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        )\n      )\n    )\n    (up): UpSample(\n      (up): Upsample(scale_factor=2.0, mode='bilinear')\n      (conv): ConvBlock(\n        (conv): Sequential(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (skip): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      )\n    )\n  )\n  (head): HeatmapHead(\n    (head): Sequential(\n      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): ReLU()\n      (2): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n)"},"metadata":{}}],"execution_count":88},{"cell_type":"code","source":"import torch\nimport numpy as np\n\ndef decode_heatmaps(heatmaps):\n    \"\"\"\n    heatmaps: (B, 4, H, W)\n    returns: (B, 4, 2)  ->  (x,y) per landmark\n    \"\"\"\n    B, C, H, W = heatmaps.shape\n    coords = []\n\n    for b in range(B):\n        sample_pts = []\n        for c in range(C):\n            hm = heatmaps[b, c]\n            y, x = torch.nonzero(hm == hm.max(), as_tuple=True)\n            x = x[0].item()\n            y = y[0].item()\n            sample_pts.append([x, y])\n        coords.append(sample_pts)\n\n    return torch.tensor(coords, dtype=torch.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T12:26:38.947429Z","iopub.execute_input":"2025-12-29T12:26:38.947725Z","iopub.status.idle":"2025-12-29T12:26:38.953602Z","shell.execute_reply.started":"2025-12-29T12:26:38.947696Z","shell.execute_reply":"2025-12-29T12:26:38.952965Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"import numpy as np\nimport torch\n\n@torch.no_grad()\ndef evaluate_on_validation(model, loader):\n    all_errors = []\n\n    model.eval()\n\n    for imgs, gt_hm in loader:\n        imgs = imgs.cuda()\n\n        pred_hm = model(imgs)\n        pred_xy = decode_heatmaps(pred_hm).cpu().numpy()\n        gt_xy   = decode_heatmaps(gt_hm).numpy()\n\n        for p, g in zip(pred_xy, gt_xy):\n            d = np.linalg.norm(p - g, axis=1)  # shape (4,)\n            all_errors.extend(d)\n\n    print(\"Validation Mean Pixel Error :\", np.mean(all_errors))\n    print(\"Validation Median Pixel Error :\", np.median(all_errors))\n    return np.array(all_errors)\n\nval_errors = evaluate_on_validation(model, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T12:26:38.954310Z","iopub.execute_input":"2025-12-29T12:26:38.954616Z","iopub.status.idle":"2025-12-29T12:26:40.689490Z","shell.execute_reply.started":"2025-12-29T12:26:38.954595Z","shell.execute_reply":"2025-12-29T12:26:40.688657Z"}},"outputs":[{"name":"stdout","text":"Validation Mean Pixel Error : 24.68644\nValidation Median Pixel Error : 4.736068\n","output_type":"stream"}],"execution_count":90}]}